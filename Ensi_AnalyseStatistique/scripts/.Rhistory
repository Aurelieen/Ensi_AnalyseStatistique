# cat("IC ", IC, "\n")
# IC
return(IC)
}
# Fonction simulant m échantillons de taille n de la loi géométrique de paramètre p, et calculant
# ensuite leur intervalle de confiance de seuil alpha pour Pn obtenu via une évaluation empirique.
# Elle affiche ensuite le taux d'intervalles de confiance contenant p sur les m calculés.
validation_intervalles <- function(m,n,p,alpha)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons = simule_geometriques(m,n,p)
# Validation des intervalles
bons_intervalles <- 0
for (k in (1:m))
{
# Calcul de l'interval de confiance
IC = intervalle(echantillons[k,],alpha)
# Vérification de la validité de l'intervalle
if (p > IC[1] & p < IC[2])
{
bons_intervalles <- bons_intervalles + 1
}
}
# cat("p ", p, "\n")
taux <- bons_intervalles/m
# cat(taux*100, "% des intervalles encadrent bien le paramètre p. Pourcentage attendu : ", (1 - alpha)*100, "%")
return(taux)
}
graphique_3_1 <- function() {
recolte = c()
recolte_critere = c()
for (p in seq(0.05, 1, by=0.05)) {
nb_i = validation_intervalles(500, 50, p, 0.05)
recolte = c(recolte, nb_i)
recolte_critere = c(recolte_critere, p)
}
plot(recolte_critere, recolte)
}
graphique_3_1()
intervalle <- function(echantillon, alpha)
{
n <- length(echantillon)
# On évalue la moyenne empirique de l'echantillon et on en déduit une estimation du paramètre
# de la loi que suivent les variables aléatoires
Xn <- mean(echantillon)
Pn <- 1/Xn
# On calcule les valeurs nécessaires à l'évaluation de l'intervalle
Ua <- qnorm(1 - alpha/2)
inter <- Ua*sqrt((Xn-1)/(n*(Xn^3)))
# On évalue l'intervalle
b_inf <- Pn - inter
b_sup <- Pn + inter
IC <- c(b_inf,b_sup)
IC
}
# Fonction simulant m échantillons de taille n de la loi géométrique de paramètre p, et calculant
# ensuite leur intervalle de confiance de seuil alpha pour Pn obtenu via une évaluation empirique.
# Elle affiche ensuite le taux d'intervalles de confiance contenant p sur les m calculés.
validation_intervalles <- function(m,n,p,alpha)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons = simule_geometriques(m,n,p)
# Validation des intervalles
bons_intervalles <- 0
for (k in (1:m))
{
# Calcul de l'interval de confiance
IC = intervalle(echantillons[k,],alpha)
# Vérification de la validité de l'intervalle
if (p >= IC[1] & p <= IC[2])
{
bons_intervalles <- bons_intervalles + 1
}
}
taux <- bons_intervalles/m
cat(taux*100, "% des intervalles encadrent bien le paramètre p. Pourcentage attendu : ", (1 - alpha)*100, "%")
}
validation_intervalles(10000,1000,0.8,0.05)
validation_intervalles(100,1000,0.8,0.05)
validation_intervalles(10000,10,0.8,0.05)
validation_intervalles(10000,1000,0.2,0.05)
validation_intervalles(10000,1000,0.8,0.25)
validation_intervalles(10000,1000,0.8,0.05)
validation_intervalles(100,1000,0.8,0.05)
validation_intervalles(10000,10,0.8,0.05)
validation_intervalles(10000,1000,0.2,0.05)
validation_intervalles(10000,1000,0.8,0.25)
# Partie 1 - Question 3
# Graphe de probabilités pour la loi géométrique
# Récupération des données
# (!) Il faut se trouver dans le dossier des scripts
# (!) Sous RStudio, menu "Session > Set Working directory > To Source File Location"
groupe1 <- read.table("groupe1.txt")[,1]
groupe2 <- read.table("groupe2.txt")[,1]
# Tri des données
groupe1_ord <- sort(groupe1)
groupe2_ord <- sort(groupe2)
# Fonction pour tracer un graphe de probabilités
# par rapport à la loi géométrique (cas r = 1)
# PRECONDITIONS : <echantillon> est trié
graphe_probabilites <- function(echantillon)
{
# Le graphe de probabilités est de la forme h[F(k)] = alpha(p)g(k) + beta(p)
# avec, dans le cas de la loi géométrique :
#   -> h[F(k)] = ln(1 - F(k))
#   -> alpha(p) = ln(1 - p)
#   -> g(k) = k
#   -> beta(p) = 0
# On trace donc (g(k_i*), h(i/n))
# NB : on évite le cas ln(0) indéfini en allant jusqu'à n - 1
n = length(echantillon)
plot(head(echantillon, -1), log(1 - seq(1:(n-1))/n), xlab="Données triées k_i* simulées", ylab="Points h(i/n) de référence")
# Affichage graphique
title("Vérification du graphe de probabilités", col.main="chartreuse4")
# Si on considère les points alignés, on peut essayer d'en déduire une pente
# qui représentera le paramètre p. On trace la droite des moindres carrés.
abs = head(echantillon,  -1)
ord = log(1 - seq(1:(n-1))/n)
reg = lm(ord~abs)
lines(abs, fitted.values(reg))
# On estime et retourne un paramètre p
# La pente de la droite des moindres carrés est ln(1 - p)
pente = reg$coefficients[2]
return(-exp(pente) + 1)
}
pg1 = graphe_probabilites(groupe1_ord)
pg2 = graphe_probabilites(groupe2_ord)
# Jeu de données simulé pour la mesure de qualité
nb_simulations = 10000;
simulations = rgeom(nb_simulations, pg1)
simulations_ord = sort(simulations)
pgs = graphe_probabilites(simulations_ord)
setwd("~/Documents/Ensimag/1A/Ensi_PMS/scripts")
# Partie 1 - Question 3
# Graphe de probabilités pour la loi géométrique
# Récupération des données
# (!) Il faut se trouver dans le dossier des scripts
# (!) Sous RStudio, menu "Session > Set Working directory > To Source File Location"
groupe1 <- read.table("groupe1.txt")[,1]
groupe2 <- read.table("groupe2.txt")[,1]
# Tri des données
groupe1_ord <- sort(groupe1)
groupe2_ord <- sort(groupe2)
# Fonction pour tracer un graphe de probabilités
# par rapport à la loi géométrique (cas r = 1)
# PRECONDITIONS : <echantillon> est trié
graphe_probabilites <- function(echantillon)
{
# Le graphe de probabilités est de la forme h[F(k)] = alpha(p)g(k) + beta(p)
# avec, dans le cas de la loi géométrique :
#   -> h[F(k)] = ln(1 - F(k))
#   -> alpha(p) = ln(1 - p)
#   -> g(k) = k
#   -> beta(p) = 0
# On trace donc (g(k_i*), h(i/n))
# NB : on évite le cas ln(0) indéfini en allant jusqu'à n - 1
n = length(echantillon)
plot(head(echantillon, -1), log(1 - seq(1:(n-1))/n), xlab="Données triées k_i* simulées", ylab="Points h(i/n) de référence")
# Affichage graphique
title("Vérification du graphe de probabilités", col.main="chartreuse4")
# Si on considère les points alignés, on peut essayer d'en déduire une pente
# qui représentera le paramètre p. On trace la droite des moindres carrés.
abs = head(echantillon,  -1)
ord = log(1 - seq(1:(n-1))/n)
reg = lm(ord~abs)
lines(abs, fitted.values(reg))
# On estime et retourne un paramètre p
# La pente de la droite des moindres carrés est ln(1 - p)
pente = reg$coefficients[2]
return(-exp(pente) + 1)
}
pg1 = graphe_probabilites(groupe1_ord)
pg2 = graphe_probabilites(groupe2_ord)
# Jeu de données simulé pour la mesure de qualité
nb_simulations = 10000;
simulations = rgeom(nb_simulations, pg1)
simulations_ord = sort(simulations)
pgs = graphe_probabilites(simulations_ord)
# Partie 1 - Question 4
# Estimations graphiques de la loi binomiale négative
# Simulation d'un grand jeu de données de rnbinom(nb_mesures, r, p)
p_r = 4
p_p = 0.4
donnees = rnbinom(1000000, p_r, p_p) + p_r
donnees_ord = sort(donnees)
# FONCTION
repartition <- function(echantillon) {
table_d = c()
for (i in 1:max(echantillon)) {
table_d[i] = as.double(length(echantillon[echantillon == i]))
}
print(table_d)
return(table_d)
}
# FONCTION
g <- function(echantillon, table_d) {
table_g = c()
ensemble_i= c()
for (i in 1:max(echantillon)) {
if (!is.na(table_d[i + 1]) & table_d[i + 1] > 0) {
table_g = c(table_g, i * table_d[i] / table_d[i + 1])
ensemble_i = c(ensemble_i, i)
}
}
return(list(table_g, ensemble_i))
}
# Dessin du nuage de points (x, g(x))
graphe_probabilites <- function(echantillon, r, p) {
table_d = repartition(echantillon)
axes = g(echantillon, table_d)
table_g = head(axes[[1]], -length(axes[[1]])/5)
ensemble_i = head(axes[[2]], -length(axes[[2]])/5)
plot(ensemble_i, table_g, xlab="Données x de loi binomiale négative simulées", ylab="g(x)")
title("Vérification de la méthode 1.4 par simulation", col.main="chartreuse4")
# Application de la régression linéaire
abs = ensemble_i
ord = table_g
reg = lm(ord~abs)
lines(abs, fitted.values(reg))
# Estimation de pg2 et pg3
a = reg$coefficients[2]
b = reg$coefficients[1]
pg2 = 1 - (1/a)
pg3 = 1 - (1 - r)/b
# Renvoi des résultats
return(list(pg2, pg3))
}
resultat = graphe_probabilites(donnees_ord, p_r, p_p)
pg2 = resultat[[1]][1]
pg3 = resultat[[2]][1]
# Coefficients
print(pg2)
print(pg3)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
# Groupe1
groupe1<-read.table("groupe1.txt")[,1]
groupe1 <- sort(groupe1)
n<-length(groupe1)
# Indicateurs statistiques
summary(groupe1)
# Moyenne, variance et écart-type empiriques (sans biais)
mean(groupe1)
var(groupe1)
sd(groupe1)
coef_var_g1 = sd(groupe1) / mean(groupe1)
coef_var_g1
# Affichage de lhistogramme
barplot(table(groupe1), xlab="Nombre de fixations pour la lecture des textes", ylab="Nombre de personnes pour ce nombre de fixations")
title("Diagramme en colonnes du nombre de fixations pour le groupe 1", col.main="chartreuse4")
# Groupe2
groupe2<-read.table("groupe2.txt")[,1]
groupe2 <- sort(groupe2)
n<-length(groupe2)
median(groupe2)
# Indicateurs statistiques
summary(groupe2)
# Moyenne, variance et écart-type empiriques (sans biais)
mean(groupe2)
var(groupe2)
sd(groupe2)
coef_var_g2 = sd(groupe2) / mean(groupe2)
coef_var_g2
# Affichage de l'histogramme
barplot(table(groupe2), xlab="Nombre de fixations pour la lecture des textes", ylab="Nombre de personnes pour ce nombre de fixations")
title("Diagramme en colonnes du nombre de fixations pour le groupe 2", col.main="chartreuse4")
Mode(groupe1)
#Estimateur
groupe1<-read.table("groupe1.txt")[,1]
groupe2<-read.table("groupe2.txt")[,1]
# Méthode des moments pour trouver l'estimateur de p
estimateur_Geo <- function(echantillon)
{
Xn <- mean(echantillon)
Sn <- sd(echantillon)
pn <- 1/Xn
#Intervalle de confiance pour p (cf question 1.2)
n <- length(echantillon)
l <- 1.96^2 * pn/n
p1 = pn - 0.5*l*(pn^2)*(1+sqrt(1+(4*(1-pn))/(l*pn)))
p2 = pn - 0.5*l*(pn^2)*(1-sqrt(1+(4*(1-pn))/(l*pn)))
print(list(pn, p1, p2))
return(list(pn, p1, p2))
}
# Méthode des moments pour trouver les estimateurs de p et de r
estimateur_BN <- function(echantillon)
{
Xn <- mean(echantillon)
Sn <- sd(echantillon)
rn <- Xn^2 / (Xn + Sn^2)
rn <- round(rn, digit=0)
pn <- rn/Xn
# print(Xn / (Xn + Sn^2))
print(list(pn, rn))
return(list(pn, rn))
}
#On approxime groupe1 à une loi géométrique
estimateur_Geo(groupe1)
#On approxime groupe1 et groupe2 à une loi binomiale négative
estimateur_BN(groupe1)
estimateur_BN(groupe2)
############################
# Vérification
############################
# FONCTION
repartition <- function(echantillon) {
table_d = c()
for (i in 1:max(echantillon)) {
table_d[i] = as.double(length(echantillon[echantillon == i]))
}
print(table_d)
return(table_d)
}
# FONCTION
g <- function(echantillon, table_d) {
table_g = c()
ensemble_i= c()
for (i in 1:max(echantillon)) {
if (!is.na(table_d[i + 1]) & table_d[i + 1] > 0) {
table_g = c(table_g, i * table_d[i] / table_d[i + 1])
ensemble_i = c(ensemble_i, i)
}
}
return(list(table_g, ensemble_i))
}
# Dessin du nuage de points (x, g(x))
graphe_probabilites <- function(echantillon, r, p) {
table_d = repartition(echantillon)
axes = g(echantillon, table_d)
table_g = head(axes[[1]], -15)
ensemble_i = head(axes[[2]], -15)
plot(ensemble_i, table_g, xlab="Données x de de loi BN(5, 0.2533)", ylab="g(x)")
title("Vérification du paramètre p par simulation", col.main="chartreuse4")
# Application de la régression linéaire
abs = ensemble_i
ord = table_g
reg = lm(ord~abs)
lines(abs, fitted.values(reg))
# Estimation de pg2 et pg3
a = reg$coefficients[2]
b = reg$coefficients[1]
pg2 = 1 - (1/a)
pg3 = 1 - (1 - r)/b
# Renvoi des résultats
return(list(pg2, pg3))
}
graphe_probabilites(groupe1, 5, 0.2533172)
# Exercice 3
# Fonction simulant m échantillons de taille n de la loi géométrique de paramètre p
simule_geometriques <- function(m,n,p)
{
# On créée un vecteur contenant les m échantillons
echantillons <- matrix(nrow = m, ncol = n)
for (k in (1:m))
{
# rgeom(n,p) simule un échantillon de taille n d'une variable aléatoire Y
# telle que Y+1 suit une loi géométrique de paramètre p
y = rgeom(n,p)
echantillons[k,] <- y+1
}
echantillons
}
#Q1 - Vérification des intervalles de confiance
# Fonction évaluant l'intervalle de confiance asymptotique de seuil alpha
# pour une échantillon donné suivant une loi géométrique
intervalle <- function(echantillon, alpha)
{
n <- length(echantillon)
# On évalue la moyenne empirique de l'echantillon et on en déduit une estimation du paramètre
# de la loi que suivent les variables aléatoires
Xn <- mean(echantillon)
Pn <- 1/Xn
# On calcule les valeurs nécessaires à l'évaluation de l'intervalle
Ua <- qnorm(1 - alpha/2)
inter <- Ua*sqrt((Xn-1)/(n*(Xn^3)))
# On évalue l'intervalle
b_inf <- Pn - inter
b_sup <- Pn + inter
IC <- c(b_inf,b_sup)
IC
}
# Fonction simulant m échantillons de taille n de la loi géométrique de paramètre p, et calculant
# ensuite leur intervalle de confiance de seuil alpha pour Pn obtenu via une évaluation empirique.
# Elle affiche ensuite le taux d'intervalles de confiance contenant p sur les m calculés.
validation_intervalles <- function(m,n,p,alpha)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons = simule_geometriques(m,n,p)
# Validation des intervalles
bons_intervalles <- 0
for (k in (1:m))
{
# Calcul de l'interval de confiance
IC = intervalle(echantillons[k,],alpha)
# Vérification de la validité de l'intervalle
if (p >= IC[1] & p <= IC[2])
{
bons_intervalles <- bons_intervalles + 1
}
}
taux <- bons_intervalles/m
cat(taux*100, "% des intervalles encadrent bien le paramètre p. Pourcentage attendu : ", (1 - alpha)*100, "%")
}
validation_intervalles(10000,1000,0.8,0.05)
validation_intervalles(100,1000,0.8,0.05)
validation_intervalles(10000,10,0.8,0.05)
validation_intervalles(10000,1000,0.2,0.05)
validation_intervalles(10000,1000,0.8,0.25)
# Q2 - Vérification de la loi faible des grands nombres
# Fonction simulant m échantillons de taille n de la loi géométrique de paramètre p, et calculant
# ensuite leur moyenne empirique. Elle affiche ensuite si le nombre de moyennes empiriques ayant
# un écart inférieur à e avec l'espérance
validation_grands_nombres <- function(m,n,p,e)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons <- simule_geometriques(m,n,p)
# Validation des moyennes empiriques
esperance <- 1/p
moyennes_justes <- 0
for (k in (1:m))
{
moyenne_empirique = mean(echantillons[k,])
if (abs(moyenne_empirique - esperance) < e)
{
moyennes_justes <- moyennes_justes + 1
}
}
cat(moyennes_justes, "échantillons (de taille",n,") sur les", m ,"simulés ont un écart entre leur espérance et leur moyenne empirique inférieur à", e, "\n")
return(moyennes_justes / m)
}
# Execution en faisant augmenter la valeur de n
valeur = validation_grands_nombres(100,500,0.7,0.05)
valeur
validation_grands_nombres(100,1000,0.7,0.05)
validation_grands_nombres(100,5000,0.7,0.05)
graphique_2 <- function() {
recolte = c()
recolte_m = c()
for (n in seq(50, 1000, by=10)) {
moyennes_justes = validation_grands_nombres(100,n,0.7,0.05)
recolte = c(recolte, moyennes_justes)
recolte_m = c(recolte_m, n)
}
plot(recolte_m, recolte, xlab="Taille de l'échantillon simulé", ylab="Pourcentage de moyennes justes pour 100 simulations")
title("Evolution de la part de moyennes justes à epsilon = 0.05", col.main="chartreuse4")
}
graphique_2()
# On peut conclure de ces résultats que la moyenne empirique se rapproche bien de l'espérance
# quand n augmente, ce qui confirme le résultat de la loi faible des grands nombre
# Q3 - Validation du théorème central limite
validation_theoreme_central <- function(m,n,p)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons <- simule_geometriques(m,n,p)
# Calcul des moyennes empiriques
moyennes <- matrix(0,1,m)
moy <- 1/p
std <- sqrt(1-p)/p
for (k in (1:m))
{
moyennes[k] <- mean(echantillons[k,])
}
# Affichage de l'histogramme à classes des moyennes empiriques ainsi que de la loi normale associée
min <- min(moyennes)
max <- max(moyennes)
brk <- seq(min,max,length = 20)
hist(moyennes,xlab = "", ylab = "Effectif", breaks = brk, main = "Histogramme des moyennes et loi normale associée")
par(new=TRUE)
plot(function(x) dnorm(x,moy,std),  moy-4*std, moy+4*std,xlab="Moyenne empirique de l'échantillon (loi géométrique)",ylab="",main = "",xaxt="n", yaxt="n", col = "red")
}
validation_theoreme_central(1000,5,0.2)
validation_theoreme_central(1000,10,0.2)
validation_theoreme_central(1000,20,0.2)
validation_theoreme_central(1000,100,0.2)
validation_theoreme_central(1000,200,0.2)
validation_theoreme_central(1000,500,0.2)
validation_theoreme_central(1000,1000,0.2)
validation_theoreme_central(1000,2000,0.2)
validation_theoreme_central(1000,5000,0.2)
# Pour un nombre d'échantillons suffisement grand (m = 1000), on voit que quand n augmente la loi de répartition
# des moyennes empiriques se rapproche d'une loi normale
# Q4 - Validation du théorème central limite
simule_BN(m,n,r,p) <- function(m,n,r,p)
{
for (k in (1:m))
{
# rgeom(n,p) simule un échantillon de taille n d'une variable aléatoire Y
# telle que Y+1 suit une loi géométrique de paramètre p
y = rnbinom(n,r,p)
echantillons[k,] <- y+1
}
}
echantillons
test_loi_BN <- function(m,n,r,p)
{
# Simulation des m échantillons de taille n et de paramètre p
echantillons <- simule_BN(m,n,r,p)
# Calcul des estimateurs pn et rn pour chaque échantillon
ecart_p = c()
ecrat_r = c()
for (k in (1:m)){
Xn_k = mean(echantillons[k,])
Sn_k = sd(echantillons[k,])
#On calcule la somme des écarts à la moyenne
####### IL FAUT FAIRE LA SOMME DES ECARTS A LA MOYENNE
####### DANS UN CAS, ET LA SOMME DES CARRES DANS L'AUTRE
ecart_p[k] = (Xn_k/(Xn_k + Sn_k)) - p
ecart_p[k] = (Xn_k^2/(Xn_k + Sn_k)) - r
}
}
